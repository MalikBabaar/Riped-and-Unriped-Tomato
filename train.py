{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4KB1JfLetkaj40YISdIjR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalikBabaar/Riped-and-Unriped-Tomato/blob/main/train_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptnNHzaXpzD5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description='Train a tomato ripeness classification model.')\n",
        "    parser.add_argument('--data_dir', type=str, required=True,\n",
        "                       help='Base directory containing Images and labels folders')\n",
        "    parser.add_argument('--img_size', type=int, default=128,\n",
        "                       help='Image width and height (default: 128)')\n",
        "    parser.add_argument('--batch_size', type=int, default=32,\n",
        "                       help='Batch size for training (default: 32)')\n",
        "    parser.add_argument('--epochs', type=int, default=30,\n",
        "                       help='Number of training epochs (default: 30)')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.0001,\n",
        "                       help='Learning rate for optimizer (default: 0.0001)')\n",
        "    parser.add_argument('--output_model', type=str, default='best_model.h5',\n",
        "                       help='Filename to save the best model (default: best_model.h5)')\n",
        "    return parser.parse_args()\n",
        "\n",
        "def parse_yolo_label(label_path):\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    if len(lines) > 0:\n",
        "        return int(lines[0].split()[0])  # First number is the class\n",
        "    return 0  # Default to unripe if no label\n",
        "\n",
        "def load_dataset(images_dir, labels_dir):\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    labels = []\n",
        "\n",
        "    for img_file in image_files:\n",
        "        label_file = os.path.join(labels_dir, os.path.splitext(img_file)[0] + '.txt')\n",
        "        label = parse_yolo_label(label_file)\n",
        "        labels.append(label)\n",
        "\n",
        "    return pd.DataFrame({'filename': image_files, 'label': labels})\n",
        "\n",
        "def create_data_generator(df, images_dir, batch_size, img_size, augment=False):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    ) if augment else ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    while True:\n",
        "        batch = df.sample(n=batch_size)\n",
        "        images = []\n",
        "        labels = []\n",
        "        for _, row in batch.iterrows():\n",
        "            img_path = os.path.join(images_dir, row['filename'])\n",
        "            img = load_img(img_path, target_size=(img_size, img_size))\n",
        "            img = img_to_array(img)\n",
        "            img = datagen.random_transform(img) if augment else img\n",
        "            img = datagen.standardize(img)\n",
        "            images.append(img)\n",
        "            labels.append(row['label'])\n",
        "        yield np.array(images), np.array(labels)\n",
        "\n",
        "def build_model(img_size, learning_rate):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, val_df, images_dir, batch_size, img_size):\n",
        "    eval_gen = create_data_generator(val_df.copy(), images_dir, batch_size, img_size, augment=False)\n",
        "    y_true = val_df['label'].values[:len(val_df) // batch_size * batch_size]\n",
        "    y_pred = model.predict(eval_gen, steps=len(val_df) // batch_size)\n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['unripe', 'ripe']))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Set dataset paths\n",
        "    images_dir = os.path.join(args.data_dir, 'Images')\n",
        "    labels_dir = os.path.join(args.data_dir, 'labels')\n",
        "\n",
        "    # Load and split dataset\n",
        "    df = load_dataset(images_dir, labels_dir)\n",
        "    print(\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "    print(\"\\nTrain samples:\", len(train_df))\n",
        "    print(\"Validation samples:\", len(val_df))\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = create_data_generator(train_df, images_dir, args.batch_size, args.img_size, augment=True)\n",
        "    val_gen = create_data_generator(val_df, images_dir, args.batch_size, args.img_size)\n",
        "\n",
        "    # Calculate steps\n",
        "    steps_per_epoch = len(train_df) // args.batch_size\n",
        "    validation_steps = len(val_df) // args.batch_size\n",
        "\n",
        "    # Build and train model\n",
        "    model = build_model(args.img_size, args.learning_rate)\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ModelCheckpoint(args.output_model, monitor='val_loss', save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=args.epochs,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Load best model and evaluate\n",
        "    model = tf.keras.models.load_model(args.output_model)\n",
        "    evaluate_model(model, val_df, images_dir, args.batch_size, args.img_size)\n",
        "    plot_history(history)\n",
        "\n",
        "    print(f\"\\nModel saved to {args.output_model}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}
